# Weekâ€¯1 Â· Dayâ€¯2 â€“ Lecture Notes: **Evaluating LLMs**

> _Stanford CS25 â€œEvaluationâ€ miniâ€‘lecture â€¢ Last updatedÂ 23â€¯Junâ€¯2025_

---

## ğŸ“š Primary Resources

- ğŸ“ºÂ **Lecture video:** [ZaQYMâ€‘YF1rM](https://www.youtube.com/watch?v=ZaQYM-YF1rM)
- ğŸ“„Â Gu etÂ al.,Â 2023Â â€” â€œJudging LLMâ€‘asâ€‘aâ€‘Judge with MTâ€‘Bench & ChatbotÂ Arenaâ€ <https://arxiv.org/abs/2306.05685>
- ğŸ“„Â Zhang etÂ al.,Â 2025Â â€” â€œA Survey on LLMâ€‘asâ€‘aâ€‘Judgeâ€ <https://arxiv.org/abs/2411.15594>

---

## 1Â Â Why Evaluation Matters

> **Goal:** Quantify progress so you canâ€¦
>
> 1. **Identify improvements**
> 2. **Select models** for a useâ€‘case
> 3. **Decide production readiness**

> ğŸ’­ **How does this differ from nonâ€‘LLM software QA?**  
> Traditional apps have binary pass/fail specs; generative models live on a quality continuum, so we need graded metrics and statistical tests.

### ğŸ› ï¸Â Core ingredients of any evaluation pipeline

- **Evaluation dataset** (prompts/instructions + references)  
- **Evaluation metric** (automatic, human, or hybrid)

### Classic ML example

> Dogâ€‘vsâ€‘notâ€‘dog classifier â‡’ closedâ€‘ended, single scalar accuracy.

---

## 2Â Â Evaluation Challenges with LLMs

| Challenge | Why itâ€™s hard | Typical workâ€‘around |
|-----------|---------------|---------------------|
| ğŸŒ **Task diversity & openâ€‘endedness** | No single â€œgroundâ€‘truthâ€ answer | Collect *many* representative prompts or convert to closed questions (but that may change the task). |
| ğŸ” **Referenceâ€‘based metrics too rigid** | BLEU/ROUGE struggle with creative tasks (â€œWrite me a bookâ€). | Use semantic similarity or human/LLM judges. |
| ğŸ‘©â€âš–ï¸ **Human evaluation** | Accurate but expensive, slow, nonâ€‘reproducible. | **LLMâ€‘asâ€‘aâ€‘Judge**: delegate scoring to a strong model. |

### Papers studying LLM judges

- **Zhangâ€¯2025** â€“ comprehensive survey  
- **Guâ€¯2023** â€“ MTâ€‘Bench, Arena agreement  
- **Sunâ€¯2024** â€“ â€œPrometheusâ€ fineâ€‘tuned judge <https://arxiv.org/abs/2404.04475>

---

## 3Â Â Academic & Open Benchmarks

### Highâ€‘level purposes

- **Development loop** (quick regression tests)  
- **Model selection & leaderboard PR**  
- **Public accountability**

### Metric note â€“ *Perplexity*

- Intuition: average branching factor over tokens.  
- âš ï¸Â Not comparable across tokenizers or corpora â†’ best used *within* one model family.

### â€œKitchenâ€‘sinkâ€ closedâ€‘task suites

- **HELM**, **HFÂ Open LLM LeaderboardÂ v2**  
  - *MMLUâ€‘Pro*: broad knowledge  
  - *GPQA*: expertâ€‘level questions  
  - *MuSR*: longâ€‘context reasoning  
  - *MATH*: highâ€‘school math  
  - *IFEval*: instructionâ€‘following format check  
  - *BBH*: 23 challenge tasks

> **Pros:** simple, oneâ€‘number score.  
> **Cons:** weak on openâ€‘ended generation.

### Humanâ€‘inâ€‘theâ€‘loop leaderboards

- **ChatbotÂ Arena** â€“ pairwise Elo from crowd votes.

### LLMâ€‘based leaderboards

- **AlpacaEval / AlpacaEvalâ€‘LC**  
  - Graphic in lecture: cost vs. agreement with humans â†’ LLM judges reach **94â€¯%** correlation (postâ€‘lengthâ€‘control **98â€¯%**).

---

## 4Â Â LLMâ€‘asâ€‘aâ€‘Judge: Pros, Cons & Biases

### âœ…Â Benefits

- Handles **openâ€‘ended** tasks  
- **Scales** cheaply & quickly  
- Quality improves with stronger backbone models

### âš ï¸Â Drawbacks

- Requires **trust** in the â€œoracleâ€ LLM  
- Potential **biases** & *spurious correlations* (e.g., length, style)  
- Some loss of manual **control / interpretability**

#### Observed biases & mitigations

| Bias | Observation | Mitigation |
|------|-------------|------------|
| **Length bias** | AlpacaEvalâ€¯1.0: longer answers won **74â€¯%** of comparisons. | Regression debiasing â†’ AlpacaEvalâ€¯2.0. |
| **Position bias** | Judges favour left/first answer. | Randomised answer order. |
| **Verbosity / selfâ€‘identification** | Judges reward model selfâ€‘praise. | Strip model IDs & enforce length cap. |

---

## 5Â Â Specialised Benchmarks & Tools

- **MTâ€‘Bench** â€“ multiâ€‘turn dialogue quality.  
- **RubricEval** â€“ scalable scoring in expert domains.

> Other open issues: data contamination, benchmark monoculture, adversarial robustness.

---

## 6Â Â Takeâ€‘home Questions

1. Which bias matters *most* for your useâ€‘case, and how will you detect it?  
2. If GPTâ€‘4o is the judge, how do you quantify drift when GPTâ€‘5 arrives?  
3. Where does human evaluation still provide irreplaceable value?

---

**End of notes**

