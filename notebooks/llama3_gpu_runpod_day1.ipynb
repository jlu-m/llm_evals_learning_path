{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83e\udde0 LLaMA 3 GPU Inference on RunPod\nSetup log for Week 2 \u2013 Day 1 of LLM Evaluation Roadmap"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# \u2705 1. System setup (RunPod template: PyTorch 2.1 + CUDA 11.8)\n!sudo apt update\n!sudo apt install -y cmake build-essential libcurl4-openssl-dev git-lfs wget\n!pip install --upgrade pip\n!pip install huggingface_hub"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# \u2705 2. Clone and build llama.cpp with CUDA\n!cd ~ && git clone https://github.com/ggerganov/llama.cpp.git\n!mkdir -p ~/llama.cpp/build\n!cd ~/llama.cpp/build && cmake .. -DGGML_CUDA=on -DLLAMA_BUILD_EXAMPLES=on && make -j"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# \u2705 3. Authenticate with Hugging Face and download model\n!huggingface-cli login  # You will need to paste your token manually\n\n# Download model (4.6GB) into llama.cpp directory\n!mkdir -p ~/llama.cpp/models/llama-3\n!huggingface-cli download QuantFactory/Meta-Llama-3-8B-Instruct-GGUF --include \"Meta-Llama-3-8B-Instruct.Q4_K_M.gguf\" --local-dir ~/llama.cpp/models/llama-3\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# \u2705 4. Run inference!\n!~/llama.cpp/build/bin/llama-cli -m ~/llama.cpp/models/llama-3/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -p \"What is the capital of France?\" -n 128 --color"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}