{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nYJ9pnTPb0u"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade evals openai"
      ],
      "metadata": {
        "id": "M8WErRfWP1VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import openai\n",
        "import evals\n",
        "\n",
        "print(\"Python:\", sys.version.splitlines()[0])\n",
        "print(\"Torch CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "# If you’re on Colab you can also run:\n",
        "#!nvidia-smi\n",
        "\n",
        "print(\"openai lib version:\", openai.__version__)\n",
        "print(\"evals version:\", evals.__version__)\n"
      ],
      "metadata": {
        "id": "iiXWkuNOTtfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.metadata\n",
        "\n",
        "print(\"evals version:\", importlib.metadata.version(\"evals\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L0xDF30U3le",
        "outputId": "663596cc-9c9b-4110-d911-8b09315e63cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evals version: 3.0.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "# Option A: If you’ve set the env var outside the notebook (preferred)\n",
        "if \"OPENAI_API_KEY\" in os.environ:\n",
        "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "else:\n",
        "    # Option B: Directly assign it here (make sure not to commit this notebook with your key in it!)\n",
        "    # os.environ[\"OPENAI_API_KEY\"] = \"sk-…YOUR_KEY_HERE…\"\n",
        "    # openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "    raise ValueError(\"Please set the OPENAI_API_KEY environment variable.\")\n",
        "\n",
        "print(\"API key loaded successfully:\", bool(openai.api_key))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqB8sdu_U8vi",
        "outputId": "56ac5a88-398c-4a78-f4e9-ba7cc913d288"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key loaded successfully: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!OPENAI_API_KEY=$OPENAI_API_KEY \\\n",
        "  python -m evals.cli.oaieval \\\n",
        "    gpt-4o-mini \\\n",
        "    arithmetic-expression \\\n",
        "    --max_samples 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD1tr6tDXBAX",
        "outputId": "af6e6bb7-3a84-4743-d479-bf64d1b66cec"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-22 16:00:28,444] [registry.py:271] Loading registry from /usr/local/lib/python3.11/dist-packages/evals/registry/evals\n",
            "[2025-06-22 16:00:28,941] [registry.py:271] Loading registry from /root/.evals/evals\n",
            "[2025-06-22 16:00:30,216] [oaieval.py:215] \u001b[1;35mRun started: 250622160030OZU7PPOC\u001b[0m\n",
            "[2025-06-22 16:00:30,218] [registry.py:271] Loading registry from /usr/local/lib/python3.11/dist-packages/evals/registry/modelgraded\n",
            "[2025-06-22 16:00:30,254] [registry.py:271] Loading registry from /root/.evals/modelgraded\n",
            "[2025-06-22 16:00:30,254] [data.py:94] Fetching /usr/local/lib/python3.11/dist-packages/evals/registry/data/arithmetic-expression/samples.jsonl\n",
            "[2025-06-22 16:00:30,254] [eval.py:36] Evaluating 5 samples\n",
            "[2025-06-22 16:00:30,259] [eval.py:144] Running in threaded mode with 10 threads!\n",
            "100% 5/5 [00:20<00:00,  4.19s/it]\n",
            "[2025-06-22 16:00:51,209] [oaieval.py:278] Found 10/10 sampling events with usage data\n",
            "[2025-06-22 16:00:51,210] [oaieval.py:230] Failed to add token usage to result: unsupported operand type(s) for +: 'int' and 'CompletionTokensDetails'. Eval results will be reported and are not affected.\n",
            "[2025-06-22 16:00:51,210] [record.py:371] Final report: {'counts/N': 5, 'score': 0.0}. Logged to /tmp/evallogs/250622160030OZU7PPOC_gpt-4o-mini_arithmetic-expression.jsonl\n",
            "[2025-06-22 16:00:51,211] [oaieval.py:236] Final report:\n",
            "[2025-06-22 16:00:51,211] [oaieval.py:238] counts/N: 5\n",
            "[2025-06-22 16:00:51,211] [oaieval.py:238] score: 0.0\n",
            "[2025-06-22 16:00:51,216] [record.py:360] Logged 15 rows of events to /tmp/evallogs/250622160030OZU7PPOC_gpt-4o-mini_arithmetic-expression.jsonl: insert_time=3.942ms\n"
          ]
        }
      ]
    }
  ]
}