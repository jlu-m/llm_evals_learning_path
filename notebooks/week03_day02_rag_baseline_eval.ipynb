{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLGAv9OZ18rg",
        "outputId": "6359ae51-d8ad-4b0f-892c-010d7cbd9012"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 36.00%\n",
            "Token-level F1: 62.85%\n"
          ]
        }
      ],
      "source": [
        "import json, re, string\n",
        "from statistics import mean\n",
        "\n",
        "def normalize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return ' '.join(text.split())\n",
        "\n",
        "def em(pred, truth):\n",
        "    return int(normalize(pred) == normalize(truth))\n",
        "\n",
        "def f1(pred, truth):\n",
        "    pred_tokens  = normalize(pred).split()\n",
        "    truth_tokens = normalize(truth).split()\n",
        "    common = set(pred_tokens) & set(truth_tokens)\n",
        "    if not common:\n",
        "        return 0.0\n",
        "    precision = len(common) / len(pred_tokens)\n",
        "    recall    = len(common) / len(truth_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "em_scores, f1_scores = [], []\n",
        "\n",
        "with open(\"data/rag_preds.jsonl\", encoding=\"utf-8\") as fp:\n",
        "    for line in fp:\n",
        "        ex = json.loads(line)\n",
        "        em_scores.append(em(ex[\"pred\"], ex[\"answer\"]))\n",
        "        f1_scores.append(f1(ex[\"pred\"], ex[\"answer\"]))\n",
        "\n",
        "print(f\"Exact Match: {mean(em_scores):.2%}\")\n",
        "print(f\"Token-level F1: {mean(f1_scores):.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, datetime\n",
        "\n",
        "metrics_path = \"data/rag_metrics_day02.json\"\n",
        "os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
        "\n",
        "metrics = {\n",
        "    \"date\": datetime.date.today().isoformat(),\n",
        "    \"exact_match\": round(mean(em_scores), 4),   # 0.3600 for you\n",
        "    \"f1\": round(mean(f1_scores), 4)             # 0.6285 for you\n",
        "}\n",
        "\n",
        "with open(metrics_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "\n",
        "print(f\"Saved Day 2 baseline metrics to {metrics_path}\")\n",
        "metrics\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy_dJDwM214B",
        "outputId": "a4c3831f-d108-4579-9f89-73c714b1d9e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Day 2 baseline metrics to data/rag_metrics_day02.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'date': '2025-07-16', 'exact_match': 0.36, 'f1': 0.6285}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}